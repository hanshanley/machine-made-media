{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fc4485",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to perturb news article data\n",
    "## This code is largely taken and adapated from https://github.com/eric-mitchell/detect-gpt/blob/main/run.py\n",
    "import transformers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72f864-9e0f-4105-8cce-eb46da520823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_mask(text, span_length, pct, ceil_pct=True):\n",
    "    buffer_size = 1\n",
    "    tokens = text.split(' ')\n",
    "    mask_string = '<<<mask>>>'\n",
    "\n",
    "    n_spans = pct * len(tokens) / (span_length + buffer_size * 2)\n",
    "    if ceil_pct:\n",
    "        n_spans = np.ceil(n_spans)\n",
    "    n_spans = int(n_spans)\n",
    "\n",
    "    n_masks = 0\n",
    "    while n_masks < n_spans:\n",
    "        #print(n_masks)\n",
    "        start = np.random.randint(0, len(tokens) - span_length)\n",
    "        end = start + span_length\n",
    "        search_start = max(0, start - buffer_size)\n",
    "        search_end = min(len(tokens), end + buffer_size)\n",
    "        if mask_string not in tokens[search_start:search_end]:\n",
    "            tokens[start:end] = [mask_string]\n",
    "            n_masks += 1\n",
    "    \n",
    "    # replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
    "    num_filled = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token == mask_string:\n",
    "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
    "            num_filled += 1\n",
    "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "    \n",
    "def count_masks(texts):\n",
    "    return [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
    "    \n",
    "def extract_fills(texts):\n",
    "    # remove <pad> from beginning of each text\n",
    "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
    "\n",
    "    # return the text in between each matched mask token\n",
    "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
    "\n",
    "    # remove whitespace around each fill\n",
    "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
    "\n",
    "    return extracted_fills\n",
    "    \n",
    "def apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    all_masked_texts = [masked_texts]*len(extracted_fills)\n",
    "    # split masked text into tokens, only splitting on spaces (not newlines)\n",
    "    tokens = [x.split(' ') for x in all_masked_texts]\n",
    "\n",
    "    n_expected = count_masks(all_masked_texts)\n",
    "\n",
    "    # replace each mask token with the corresponding fill\n",
    "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
    "        if len(fills) < n:\n",
    "            tokens[idx] = []\n",
    "        else:\n",
    "            for fill_idx in range(n):\n",
    "                text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
    "\n",
    "    # join tokens back into text\n",
    "    texts = [\" \".join(x) for x in tokens]\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35dbd043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a828fcbaa24938b091974cbdf474d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask_filling_model_name = 'google/t5-v1_1-xl'\n",
    "import re\n",
    "import torch\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1031e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tokenizer = transformers.AutoTokenizer.from_pretrained(mask_filling_model_name)m\n",
    "mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(mask_filling_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b65d56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '\n",
    "span_length = 5 ## Specifies span length for model to mask \n",
    "# N = Maximum Text Length"
    "pct = 0.25 ## Specifis the percentage of data that should be masked\n",
    "masked_and_tokenized = tokenize_and_mask(' '.join(text.split()[:N]), span_length,pct)\n",
    "n_expected = count_masks([masked_and_tokenized])\n",
    "stop_id = mask_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "\n",
    "\n",
    "tokens = mask_tokenizer([masked_and_tokenized], return_tensors=\"pt\", padding=True).to(device)\n",
    "outputs = mask_model.generate(**tokens, max_length=512, do_sample=True, num_return_sequences=5, eos_token_id=stop_id)\n",
    "masked_answeres = mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "fills = extract_fills(masked_answeres)\n",
    "filled_out = apply_extracted_fills(masked_and_tokenized,fills)\n",
    "print(filled_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
